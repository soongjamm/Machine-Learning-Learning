# ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œì¸ ê²½ìš°.

import numpy as np
''  # ì…ë ¥ìœ¼ë¡œ ê³µë¶€ì‹œê°„ì„ ë„£ì—ˆì„ ë•Œ í•©ê²©/ë¶ˆí•©ê²© ì—¬ë¶€ë¥¼ íŒë‹¨í•œë‹¤.

''  # 1. í•™ìŠµë°ì´í„°(Training Data) ì¤€ë¹„

# x_data : ì˜ˆìŠµì‹œê°„, ë³µìŠµì‹œê°„
# t_data : í•©ê²©/ë¶ˆí•©ê²© ì—¬ë¶€ (0:ë¶ˆí•©ê²© | 1:í•©ê²©)
x_data = np.array([[2, 4], [4, 11], [6, 6], [8, 5], [10, 7],
                   [12, 16], [14, 8], [16, 3], [18, 7]])
t_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1]).reshape(9, 1)

# ë°ì´í„° ì°¨ì› ë° shape í™•ì¸
print("x_data.ndim = ", x_data.ndim, ", x_data.shape = ", x_data.shape)
print("t_data.ndim = ", t_data.ndim, ", t_data.shape = ", t_data.shape)


''  # 2. ì„ì˜ì˜ ì§ì„  z = Wx + b ì •ì˜ (ì„ì˜ì˜ ê°’ìœ¼ë¡œ ê°€ì¤‘ì¹˜ W, ë°”ì´ì–´ìŠ¤ b ì´ˆê¸°í™”)
W = np.random.rand(2, 1)  # 2X1 í–‰ë ¬
b = np.random.rand(1)  # 1ì°¨ì› ë°°ì—´, 1ê°œ
print("W = ", W, ", W.shape = ", W.shape, ", b : ", b, ", b.shape = ", b.shape)


''  # 3. ì†ì‹¤í•¨ìˆ˜ E(W,b) ì •ì˜
# ìµœì¢…ì¶œë ¥ì€ y = sigmoid(Wx+b) ì´ë©°, ì†ì‹¤í•¨ìˆ˜ëŠ” cross-entropy ë¡œ ë‚˜íƒ€ëƒ„


def sigmoid(x):
    return 1 / (1+np.exp(-x))


def loss_func(x, t):

    delta = 1e-7  # log ë¬´í•œëŒ€ ë°œì‚° ë°©ì§€

    z = np.dot(x, W) + b  # dot() : í–‰ë ¬ê³±
    y = sigmoid(z)

    # cross_entropy
    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))


''  # 4. ìˆ˜ì¹˜ë¯¸ë¶„ numerical_derivative ë° utility í•¨ìˆ˜ ì •ì˜
# errorval - ì†ì‹¤í•¨ìˆ˜ ê°’ì„ ë‚˜íƒ€ëƒ„
# predict - ë¯¸ë˜ ê°’ì„ ì•Œë ¤ì¤Œ
# sigmoid ê°’ì´ 0.5 ì´ìƒì´ë©´ 1, ì´í•˜ì´ë©´ 0


def numerical_derivative(f, x):  # fëŠ” í•¨ìˆ˜, xëŠ” Wë‚˜ bê°€ ì˜¨ë‹¤.
    delta_x = 1e-4  # 0.0001
    grad = np.zeros_like(x)

    # xì˜ ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ëŒë©´ì„œ ê³„ì‚°
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])

    while not it.finished:
        # í•¨ìˆ˜ ê°’ì„ x+hì—ì„œ ê³„ì‚°
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + delta_x
        fx1 = f(x)  # f(x+delta_x)

        # í¸ë¯¸ë¶„ ê³„ì‚°
        x[idx] = tmp_val - delta_x
        fx2 = f(x)  # f(x-delta_x)
        grad[idx] = (fx1 - fx2) / (2*delta_x)

        x[idx] = tmp_val
        it.iternext()

    return grad


def error_val(x, t):
    delta = 1e-7  # log ë¬´í•œëŒ€ ë°œì‚° ë°©ì§€

    z = np.dot(x, W) + b
    y = sigmoid(z)

    # cross-entropy
    return -np.sum(t * np.log(y+delta)+(1-t)*np.log((1-y)+delta))


def predict(x):
    z = np.dot(x, W) + b
    y = sigmoid(z)
    if y > 0.5:
        result = 1
    else:
        result = 0

    return y, result


''  # 5. í•™ìŠµìœ¨(learning rate) ì´ˆê¸°í™” ë° ì†ì‹¤í•¨ìˆ˜ê°€ ìµœì†Œê°€ ë  ë•Œ ê¹Œì§€ W, b ì—…ë°ì´íŠ¸
learning_rate = 1e-2  # ë°œì‚°í•˜ëŠ” ê²½ìš°, ie-3 ~ ie-6ìœ¼ë¡œ ë³€ê²½


def f(x): return loss_func(x_data, t_data)


print("initial error value = ", error_val(x_data, t_data),
      "Initial W = ", W, "\n", ", b = ", b)

for step in range(80001):

    W -= learning_rate * numerical_derivative(f, W)

    b -= learning_rate * numerical_derivative(f, b)

    if (step % 400 == 0):
        print("step = ", step, "error value = ", error_val(
            x_data, t_data), ", W = ", W, ", b = ", b)


(real_val, logical_val) = predict([3, 10])
print(real_val, logical_val)
