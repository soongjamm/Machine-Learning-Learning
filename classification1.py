import numpy as np
''  # ì…ë ¥ìœ¼ë¡œ ê³µë¶€ì‹œê°„ì„ ë„£ì—ˆì„ ë•Œ í•©ê²©/ë¶ˆí•©ê²© ì—¬ë¶€ë¥¼ íŒë‹¨í•œë‹¤.

''  # 1. í•™ìŠµë°ì´í„°(Training Data) ì¤€ë¹„

# x_data : ê³µë¶€ì‹œê°„
# t_data : í•©ê²©/ë¶ˆí•©ê²© ì—¬ë¶€ (0:ë¶ˆí•©ê²© | 1:í•©ê²©)
x_data = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]).reshape(10, 1)
t_data = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1]).reshape(10, 1)

''  # 2. ì„ì˜ì˜ ì§ì„  z = Wx + b ì •ì˜ (ì„ì˜ì˜ ê°’ìœ¼ë¡œ ê°€ì¤‘ì¹˜ W, ë°”ì´ì–´ìŠ¤ b ì´ˆê¸°í™”)
W = np.random.rand(1, 1)  # ë°°ì—´ì•ˆì— ë°°ì—´ë¡œ ëœ ì›ì†Œê°€ í•˜ë‚˜. (1x1)
b = np.random.rand(1)  # 1ì°¨ì› ë°°ì—´, 1ê°œ
print("W = ", W, ", W.shape = ", W.shape, ", b : ", b, ", b.shape = ", b.shape)


''  # 3. ì†ì‹¤í•¨ìˆ˜ E(W,b) ì •ì˜
# ìµœì¢…ì¶œë ¥ì€ y = sigmoid(Wx+b) ì´ë©°, ì†ì‹¤í•¨ìˆ˜ëŠ” cross-entropy ë¡œ ë‚˜íƒ€ëƒ„

# ì„ì˜ì˜ ì§ì„ ê°’ zê°€ sigmoidì˜ ì¸ìë¡œ ë“¤ì–´ê°€ì„œ íŒë³„ëœë‹¤.
# ê·¸ë¦¬ê³  ë’¤ì— ê³¼ì •ì—ì„œ sigmoid ë¦¬í„´ ê°’ê³¼ ì •ë‹µê³¼ ë¹„êµí•œë‹¤.
# ì†ì‹¤í•¨ìˆ˜ëŠ” ì‹¤ì œê°’ê³¼ì˜ ì˜¤ì°¨ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ë¼ê³  ë³´ë©´ ëœë‹¤.
# ì†ì‹¤í•¨ìˆ˜ëŠ” í‰ê· ì œê³± ì˜¤ì°¨ì™€ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ê°€ ìˆëŠ”ë°, í‰ê· ì œê³± ì˜¤ì°¨ëŠ” íšŒê·€ ë¶„ì„ì—ì„œ ì‚¬ìš©í•¨.
# ì˜¤ì°¨ë¥¼ ê°€ì¥ ì‘ê²Œ ë§Œë“œëŠ”ê²Œ ëª©í‘œì„
# z = Wx + b, y = sigmoid(z)
# E(W,b) = -(sigma n, i=1) {t.i log(y.i)log(1-y.i)} --> cross-entropy (ê³µì‹ ìœ ë„ëŠ” íŒ¨ìŠ¤..)


def sigmoid(x):
    return 1 / (1+np.exp(-x))


def loss_func(x, t):

    delta = 1e-7  # log ë¬´í•œëŒ€ ë°œì‚° ë°©ì§€

    z = np.dot(x, W) + b  # dot() : í–‰ë ¬ê³±
    y = sigmoid(z)

    # cross_entropy
    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))


''  # 4. ìˆ˜ì¹˜ë¯¸ë¶„ numerical_derivative ë° utility í•¨ìˆ˜ ì •ì˜
# errorval - ì†ì‹¤í•¨ìˆ˜ ê°’ì„ ë‚˜íƒ€ëƒ„
# predict - ë¯¸ë˜ ê°’ì„ ì•Œë ¤ì¤Œ
# sigmoid ê°’ì´ 0.5 ì´ìƒì´ë©´ 1, ì´í•˜ì´ë©´ 0


def numerical_derivative(f, x):  # fëŠ” í•¨ìˆ˜, xëŠ” Wë‚˜ bê°€ ì˜¨ë‹¤.
    delta_x = 1e-4  # 0.0001
    grad = np.zeros_like(x)

    # xì˜ ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ëŒë©´ì„œ ê³„ì‚°
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])

    while not it.finished:
        # í•¨ìˆ˜ ê°’ì„ x+hì—ì„œ ê³„ì‚°
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + delta_x
        fx1 = f(x)  # f(x+delta_x)

        # í¸ë¯¸ë¶„ ê³„ì‚°
        x[idx] = tmp_val - delta_x
        fx2 = f(x)  # f(x-delta_x)
        grad[idx] = (fx1 - fx2) / (2*delta_x)

        x[idx] = tmp_val
        it.iternext()

    return grad


def error_val(x, t):
    delta = 1e-7  # log ë¬´í•œëŒ€ ë°œì‚° ë°©ì§€

    z = np.dot(x, W) + b
    y = sigmoid(z)

    # cross-entropy
    return -np.sum(t * np.log(y+delta)+(1-t)*np.log((1-y)+delta))


def predict(x):

    z = np.dot(x, W) + b
    y = sigmoid(z)

    if y > 0.5:
        result = 1
    else:
        result = 0

    return y, result


''  # 5. í•™ìŠµìœ¨(learning rate) ì´ˆê¸°í™” ë° ì†ì‹¤í•¨ìˆ˜ê°€ ìµœì†Œê°€ ë  ë•Œ ê¹Œì§€ W, b ì—…ë°ì´íŠ¸
learning_rate = 1e-3  # ë°œì‚°í•˜ëŠ” ê²½ìš°, ie-3 ~ ie-6ìœ¼ë¡œ ë³€ê²½


def f(x): return loss_func(x_data, t_data)  # f(x) = loss_func(x_data, t_data)


for step in range(10001):

    W -= learning_rate * numerical_derivative(f, W)

    b -= learning_rate * numerical_derivative(f, b)
    if (step % 400 == 0):
        print("step = ", step, "error value = ", error_val(
            x_data, t_data), ", W = ", W, ", b = ", b)


(real_val, logical_val) = predict(3)
print(real_val, logical_val)
